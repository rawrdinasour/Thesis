{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/usr/local/lib/python3.9/site-packages\")\n",
    "\n",
    "import nltk, pandas as pd, numpy as np\n",
    "import re\n",
    "from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser\n",
    "from nltk.tree import ParentedTree\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "from nrclex import NRCLex\n",
    "\n",
    "#nltk.download('vader_lexicon') \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "import pickle\n",
    "from fuzzywuzzy import fuzz\n",
    "#nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_parser = CoreNLPDependencyParser(url='http://0.0.0.0:9000')\n",
    "pos_tagger = CoreNLPParser(url='http://0.0.0.0:9000', tagtype='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence (input_sent):\n",
    "    # Parse sentence using Stanford CoreNLP Parser\n",
    "    pos_type = pos_tagger.tag(input_sent.split())\n",
    "    parse_tree, = ParentedTree.convert(list(pos_tagger.parse(input_sent.split()))[0])\n",
    "    dep_type, = ParentedTree.convert(dep_parser.parse(input_sent.split()))\n",
    "    return pos_type, parse_tree, dep_type\n",
    "\n",
    "def multi_liaison (input_sent, output=['tagging','parse_tree','type_dep','spo','relation']):\n",
    "    pos_type, parse_tree, dep_type = convert_sentence(input_sent)\n",
    "    pos_sent = ' '.join([x[0]+'/'+x[1] for x in pos_type])\n",
    "    # Extract subject, predicate and object\n",
    "    subject, adjective = get_subject(parse_tree)\n",
    "    predicate = get_predicate(parse_tree)\n",
    "    objects = get_object(parse_tree)\n",
    "    # Generate the relations between subjects and objects\n",
    "    relation = get_relationship(dep_type, subject, predicate, objects)\n",
    "    if 'tagging' in output:\n",
    "        print('---TAGGING---')\n",
    "        print(pos_sent)\n",
    "        print()\n",
    "    if 'parse_tree' in output:\n",
    "        print('---PARSE TREE---')\n",
    "        parse_tree.pretty_print()\n",
    "        print()\n",
    "    if 'type_dep' in output:\n",
    "        print('---TYPED DEPENDENCIES---')\n",
    "        li = []\n",
    "        for x in dep_type.triples(): li.append(list(x))\n",
    "        return li\n",
    "        print()\n",
    "    if 'spo' in output:\n",
    "        print('---MULTI-LIAISON OUTPUT---')\n",
    "        print('Subject: ',len(subject))\n",
    "        for x in subject: print(' '.join(x))\n",
    "        print('Predicate: ',len(predicate))\n",
    "        for x in predicate: print(' '.join(x))\n",
    "        print('Object: ',len(objects))\n",
    "        for x in objects: print(' '.join(x))\n",
    "        print()\n",
    "    if 'relation' in output:\n",
    "        print('---RELATIONSHIP---')\n",
    "        for x in relation: print(x)\n",
    "\n",
    "def get_subject (parse_tree):\n",
    "    # Extract the nouns and adjectives from NP_subtree which is before the first / main VP_subtree\n",
    "    subject, adjective = [],[]\n",
    "    for s in parse_tree:\n",
    "        if s.label() == 'NP':\n",
    "            for t in s.subtrees(lambda y: y.label() in ['NN','NNP','NNS','NNPS','PRP']):\n",
    "                # Avoid empty or repeated values\n",
    "                if t.pos()[0] not in subject:\n",
    "                    subject.append(t.pos()[0])\n",
    "            for t in s.subtrees(lambda y: y.label().startswith('JJ')):\n",
    "                if t.pos()[0] not in adjective:\n",
    "                    adjective.append(t.pos()[0])\n",
    "    return subject, adjective\n",
    "\n",
    "def get_predicate (parse_tree):\n",
    "    # Extract the verbs from the VP_subtree\n",
    "    predicate = []\n",
    "    for s in parse_tree.subtrees(lambda x: x.label() == 'VP'):\n",
    "        for t in s.subtrees(lambda y: y.label().startswith('VB')):\n",
    "            if t.pos()[0] not in predicate:\n",
    "                predicate.append(t.pos()[0]) \n",
    "    return predicate\n",
    "\n",
    "def get_object (parse_tree):\n",
    "    # Extract the nouns from VP_NP_subtree\n",
    "    objects, output = [],[]\n",
    "    for s in parse_tree.subtrees(lambda x: x.label() == 'VP'):\n",
    "        for t in s.subtrees(lambda y: y.label() == 'NP'):\n",
    "            for u in t.subtrees(lambda z: z.label() in ['NN','NNP','NNS','NNPS','PRP$']):\n",
    "                output = u.pos()[0]\n",
    "                if u.left_sibling() is not None and u.left_sibling().label().startswith('JJ'):\n",
    "                    output += u.left_sibling().pos()[0]\n",
    "                if output not in objects:\n",
    "                    objects.append(output)\n",
    "    return objects\n",
    "\n",
    "def get_relationship (dep_type, subject, predicate, objects):\n",
    "    # Generate relations based on the relationship dependencies obtained from parse_tree.triples()\n",
    "    subject = [x[0] for x in subject]\n",
    "    predicate = [x[0] for x in predicate]\n",
    "    objects = [x[0] for x in objects]     \n",
    "    d1, d2, r1, r2, relation, s1, s2, subjs = [],[],[],[],[],[],[],[]\n",
    "    w1, w2, output = '','',''\n",
    "    for head, rel, dep in dep_type.triples():\n",
    "        if rel in ['nsubj','acl:relcl','conj']:\n",
    "            s1, s2 = head[0], dep[0]\n",
    "            if s2 in subject and s1 in predicate:\n",
    "                w1, w2 = s2, s1\n",
    "            elif s2 in predicate and (s1 in subject or s1 in objects):\n",
    "                w1, w2 = s1, s2\n",
    "            elif s2 in subject and s1 in subject:\n",
    "                subjs = [s1, s2]\n",
    "            if w1 != '' and w2 != '':\n",
    "                r1 = [w1, w2]\n",
    "        if rel in ['dobj','prep','nmod','conj']:\n",
    "            d1, d2 = head[0], dep[0]\n",
    "            if d1 in objects and d2 in objects: \n",
    "                r2 = [d1,d2]\n",
    "            elif d2 in objects:\n",
    "                r2 = [d2]\n",
    "            elif d1 in objects:\n",
    "                r2 = [d1]\n",
    "        if len(r1) != 0 and len(r2) != 0 and (r2[0] not in r1 and r2[-1] not in r1):\n",
    "            if len(subjs) != 0:\n",
    "                for n in subjs:\n",
    "                    output = '-'.join([n] + r1[-1:] + r2)\n",
    "                    if output not in relation:\n",
    "                        relation.append(output)\n",
    "            else:\n",
    "                output = '-'.join(r1+r2)\n",
    "                if output not in relation:\n",
    "                    relation.append(output)  \n",
    "    rm = [x for x in relation for y in relation if x != y and re.match(x,y)]\n",
    "    for x in rm: \n",
    "        if x in relation:\n",
    "            relation.remove(x)    \n",
    "    return relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoreNLP Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TYPED DEPENDENCIES---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('test', 'NN'), 'nsubj', ('This', 'DT')],\n",
       " [('test', 'NN'), 'cop', ('is', 'VBZ')],\n",
       " [('test', 'NN'), 'det', ('a', 'DT')],\n",
       " [('test', 'NN'), 'punct', ('.', '.')]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_liaison(\"This is a test.\", output='type_dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(narrative):\n",
    "    li = tokenize.sent_tokenize(narrative)\n",
    "    cleaned = []\n",
    "    \n",
    "    for sentence in li:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence)\n",
    "        sentence = sentence.replace(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|rt|\\d+', '')\n",
    "        sentence = sentence.replace(r'^\\s+|\\s+$', '') \n",
    "        sentence = sentence.replace(r'[^\\w]', ' ')\n",
    "        sentence = ' '.join([w for w in sentence.split() if w not in (stopwords)])\n",
    "        cleaned.append(sentence)\n",
    "        \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_paragraph(narrative, column_name):\n",
    "    df['paragraphs'] = df[column_name].fillna('')\n",
    "    df['paragraphs'] = df['paragraphs'].str.lower()\n",
    "    df['paragraphs'] = df['paragraphs'].str.replace(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|rt|\\d+', '')\n",
    "    df['paragraphs'] = df['paragraphs'].str.replace(r'^\\s+|\\s+$', '') \n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda x: ' '.join([w for w in x.split() if w not in (stopwords)]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"lexicon\", \"rb\")\n",
    "lexicon = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = list(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_behavior_breakdown(string):\n",
    "    li = multi_liaison(string, output=\"type_dep\")\n",
    "    behaviors = {}\n",
    "    for group in li:\n",
    "        if (group[0][1].startswith('JJ') or group[0][1].startswith('VB')) and group[0][0] not in behaviors:\n",
    "            behaviors[group[0][0]] = []\n",
    "        if group[2][1].startswith('RB') and group[0][0] in behaviors.keys():\n",
    "            behaviors[group[0][0]].append(group[2][0])\n",
    "    return behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: check if behaviors are in lexicon\n",
    "def check_behaviors(behaviors, lexicon, threshold):\n",
    "    \n",
    "#     dictionary = get_behavior_breakdown(narrative)\n",
    "    \n",
    "    for word, modifier in behaviors.items():\n",
    "        if \"not\" in modifier:\n",
    "            continue\n",
    "        else:\n",
    "            for behavior in lexicon:\n",
    "                if fuzz.ratio(word, behavior) >= threshold:\n",
    "                    return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(string):\n",
    "    result = sid.polarity_scores(string)\n",
    "    if (result['compound'] > 0):\n",
    "        return \"POS\"\n",
    "    elif (result['compound'] == 0):\n",
    "        return \"NEU\"\n",
    "    else:\n",
    "        return \"NEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_breakdown(string):\n",
    "    text_object = NRCLex(string)\n",
    "    frequencies = text_object.affect_frequencies\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(sentiment, behavior):\n",
    "    if (sentiment == \"NEG\" or behavior == True):\n",
    "        return \"UNWELL\"\n",
    "    else:\n",
    "        return \"WELL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_v2(row, narrative, sentences):\n",
    "    \n",
    "    print(row.name)\n",
    "    dictionary = {}\n",
    "    true = 0\n",
    "    false = 0\n",
    "    _hasBehavior = False\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        behaviors = get_behavior_breakdown(sentence)\n",
    "        hasBehavior = check_behaviors(behaviors, lexicon, 90)\n",
    "\n",
    "        if hasBehavior == True:\n",
    "            true+=1\n",
    "        else:\n",
    "            false+=1\n",
    "\n",
    "        dictionary.update(behaviors)\n",
    "        \n",
    "    sentiments = get_sentiment_breakdown(narrative)\n",
    "    sentiment_val = get_sentiment(narrative)\n",
    "    \n",
    "    if true > false:\n",
    "        _hasBehavior = True\n",
    "    else:\n",
    "        _hasBehavior = False\n",
    "    \n",
    "    if (sentiment_val == \"NEG\" or _hasBehavior == True):\n",
    "        return pd.Series([\"U\", dictionary, sentiments])\n",
    "    else:\n",
    "        return pd.Series([\"W\", None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"gold_standard\", \"rb\")\n",
    "df = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentences'] = df.apply(lambda x: clean_sentence(x['selftext']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-dd98175ccba2>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['paragraphs'] = df['paragraphs'].str.replace(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|rt|\\d+', '')\n",
      "<ipython-input-64-dd98175ccba2>:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['paragraphs'] = df['paragraphs'].str.replace(r'^\\s+|\\s+$', '')\n"
     ]
    }
   ],
   "source": [
    "df = clean_paragraph(df, \"selftext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['someone know victim domestic violence?',\n",
       " 'ever wanted help know how?',\n",
       " 'goal setup non-profit organization helps domestic violence victims.',\n",
       " '**the freedom train** non-profit organization help domestic violence victims escape abusers.',\n",
       " 'currently need volunteers everywhere severely lacking west coast.¬† many different options helping diving victims safe places, boarding animals, sheltering victims, coordinating shelters,handing supplies, place fro people rest/shower/eat please join group interested answer membership questions fill intake need help feel free pm :) [']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentences.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "1\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "2\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "5\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "6\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "7\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "8\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "9\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "12\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "15\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "17\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "19\n",
      "---TYPED DEPENDENCIES---\n",
      "20\n",
      "---TYPED DEPENDENCIES---\n",
      "21\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "22\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "23\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "25\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "26\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "27\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "29\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "31\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "33\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "37\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "38\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "42\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "46\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "54\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "56\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "58\n",
      "---TYPED DEPENDENCIES---\n",
      "65\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "76\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "78\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "79\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "81\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "82\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "83\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n",
      "---TYPED DEPENDENCIES---\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: http://0.0.0.0:9000/?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cssplit%2Cpos%22%2C+%22ssplit.isOneSentence%22%3A+%22true%22%7D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-f6e4c9716df5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'behaviors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentiments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraphs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7763\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7764\u001b[0m         )\n\u001b[0;32m-> 7765\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-f6e4c9716df5>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'behaviors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentiments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraphs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-3c1455fcab7b>\u001b[0m in \u001b[0;36mcheck_v2\u001b[0;34m(row, narrative, sentences)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbehaviors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_behavior_breakdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mhasBehavior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_behaviors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbehaviors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-6eabd060d6ab>\u001b[0m in \u001b[0;36mget_behavior_breakdown\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_behavior_breakdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_liaison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"type_dep\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbehaviors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JJ'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'VB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbehaviors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-414f7683e1f7>\u001b[0m in \u001b[0;36mmulti_liaison\u001b[0;34m(input_sent, output)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmulti_liaison\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tagging'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'parse_tree'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'type_dep'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'spo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'relation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpos_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdep_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpos_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Extract subject, predicate and object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-414f7683e1f7>\u001b[0m in \u001b[0;36mconvert_sentence\u001b[0;34m(input_sent)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_sentence\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Parse sentence using Stanford CoreNLP Parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpos_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mparse_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParentedTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdep_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParentedTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    364\u001b[0m         ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]\n\u001b[1;32m    365\u001b[0m         \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraw_tag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# Converting list(list(str)) -> list(str)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_tag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# Converting list(list(str)) -> list(str)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_tag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mraw_tag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mdefault_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"annotators\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mtagged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m             yield [\n\u001b[1;32m    389\u001b[0m                 [\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mapi_call\u001b[0;34m(self, data, properties, timeout)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http://0.0.0.0:9000/?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cssplit%2Cpos%22%2C+%22ssplit.isOneSentence%22%3A+%22true%22%7D"
     ]
    }
   ],
   "source": [
    "df[['tag', 'behaviors', 'sentiments']] = df.apply(lambda x: check_v2(x, x['paragraphs'], x['sentences']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rang police first instead, somehow managed find ringing them, cancelled told simple domestic seen police interceptors ect doesnt matter, left again, luckily hours digging bushes found half ripped bag included belongings, despite asking go home followed home i'm next garages ready sleep outside alone.\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentences.iloc[83][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-112-825535cc6232>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-112-825535cc6232>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \"rang police first instead, somehow managed find ringing them, cancelled told simple domestic seen police interceptors ect doesnt matter, left again, luckily hours digging bushes found half ripped bag included belongings, despite asking go home followed home i'm next garages ready sleep outside alone.\"')\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "get_behavior_breakdown('easily misundetsnding drunk found ragged around bag snapped, stuff inside emptied, lost passport, bankcard, cigarettes, id bankcard included, already blocked number, form contact previous argument, ready sleep bush, alone.',\n",
    " \"rang police first instead, somehow managed find ringing them, cancelled told simple domestic seen police interceptors ect doesnt matter, left again, luckily hours digging bushes found half ripped bag included belongings, despite asking go home followed home i'm next garages ready sleep outside alone.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
